{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptrón multicapa\n",
        "## Para el curso de Inteligencia Artificial\n",
        "### Alan García Zermeño\n",
        "12 de marzo de 2023\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WR21y7w5bK8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "#Vector One-Hot\n",
        "def onehot(v,mdi=None):\n",
        "  if mdi is None:\n",
        "    mdi = v.max()+1\n",
        "  b = np.zeros((v.size,mdi))\n",
        "  b[np.arange(v.size), v] = 1\n",
        "  return b\n",
        "\n",
        "#Entropía cruzada\n",
        "def CrEntropia(pr,trues):\n",
        "  epsilon = 1e-12\n",
        "  pr = np.clip(pr, epsilon, 1-epsilon)\n",
        "  ce = -np.sum(trues*np.log(pr+epsilon))/pr.shape[0]\n",
        "  return ce\n",
        "\n",
        "#Función de activación\n",
        "def activacion(x):\n",
        "  return 1.0/(1.0+np.exp(-x))"
      ],
      "metadata": {
        "id": "ar3pJb1fv_pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Front-propagation\n",
        "def F_propagation(x,W1,W2):    \n",
        "  z = activacion(np.dot(x, W1))\n",
        "  pog = activacion(np.dot(z,W2))\n",
        "  return pog,z\n",
        "\n",
        "#Back-propagation\n",
        "def B_propagation(x,y,learning_rate,output_s,pog,z,W1,W2):\n",
        "  #ultima capa\n",
        "  aux = onehot(y,mdi = output_s)\n",
        "  delta_last = aux - pog      \n",
        "  grad_W2 = np.dot(z.T,delta_last)\n",
        "        \n",
        "  #Capa oculta\n",
        "  grad_tmp = np.multiply(z,(1-z))\n",
        "  b = np.dot(delta_last,W2.T)\n",
        "\n",
        "  Hdelta = np.multiply(b,grad_tmp)\n",
        "  grad_W1 = np.dot(x.T,Hdelta)/x.shape[0]\n",
        "  grad_W2 = grad_W2/x.shape[0]\n",
        "\n",
        "  W1 += learning_rate*grad_W1       \n",
        "  W2 += learning_rate*grad_W2\n",
        "  return W1,W2\n",
        "\n",
        "#Loss\n",
        "def perdida(x,y,W1,W2):\n",
        "  tmp,z = F_propagation(x,W1,W2)\n",
        "  return CrEntropia(tmp,onehot(y))\n",
        "\n",
        "#-------------------------MLP-------------------------#\n",
        "def MLperceptron(x,y,epochs,batch_size,learning_rate):\n",
        "  hidden_s = 128\n",
        "  input_s = 4\n",
        "  output_s = 3\n",
        "  W1 = np.random.randn(input_s,hidden_s)\n",
        "  W2 = np.random.randn(hidden_s,output_s)  \n",
        "\n",
        "  for e in range(epochs):\n",
        "    if e%10 == 0:\n",
        "      print(e, \"----->\",\"Perdida:\",perdida(x,y,W1,W2))\n",
        "\n",
        "    idxs = np.random.permutation(len(x)) #shuffle\n",
        "\n",
        "    for i in range(0,len(x),batch_size):\n",
        "      idx = idxs[i:i+batch_size]\n",
        "      xx = x[idx]\n",
        "      yy = y[idx]\n",
        "      \n",
        "      pog,z = F_propagation(xx,W1,W2)\n",
        "      W1,W2 = B_propagation(xx,yy,learning_rate,output_s,pog,z,W1,W2)\n",
        "\n",
        "  return W1,W2"
      ],
      "metadata": {
        "id": "IfoCW320wEde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tomamos los datos del dataset de Iris, con tres clases distintas y usamos el perceptrón multicapa creado para entrenar un 80% de los datos y testear con un 20%."
      ],
      "metadata": {
        "id": "4U5Za_J5b3qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "Y = iris.target\n",
        "\n",
        "idx = np.random.permutation(Y.size)\n",
        "aux =  int(0.2*Y.size)\n",
        "\n",
        "X_train = X[idx[aux:],:]\n",
        "X_test = X[idx[:aux],:]\n",
        "Y_train = Y[idx[aux:]]\n",
        "Y_test = Y[idx[:aux]]"
      ],
      "metadata": {
        "id": "wg5VOy0JwJov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1,W2 = MLperceptron(X_train, Y_train, epochs=300, batch_size = 10, learning_rate = 1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL9uW1kOwRRO",
        "outputId": "9bf5368a-c736-478b-96b3-20160f41143b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -----> Perdida: 0.0004844928521391868\n",
            "10 -----> Perdida: 0.4566370338576269\n",
            "20 -----> Perdida: 0.3910276102927728\n",
            "30 -----> Perdida: 0.33304190705233855\n",
            "40 -----> Perdida: 0.3055907702942063\n",
            "50 -----> Perdida: 0.2890645215028086\n",
            "60 -----> Perdida: 0.280883508513037\n",
            "70 -----> Perdida: 0.27689423937827357\n",
            "80 -----> Perdida: 0.2477647016639983\n",
            "90 -----> Perdida: 0.23696380783064958\n",
            "100 -----> Perdida: 0.2383793897513628\n",
            "110 -----> Perdida: 0.229260888023191\n",
            "120 -----> Perdida: 0.21791699659367156\n",
            "130 -----> Perdida: 0.21185392196507571\n",
            "140 -----> Perdida: 0.21369456950115975\n",
            "150 -----> Perdida: 0.19233575053031954\n",
            "160 -----> Perdida: 0.20197316196487017\n",
            "170 -----> Perdida: 0.18552742242761944\n",
            "180 -----> Perdida: 0.17520335087669128\n",
            "190 -----> Perdida: 0.1770249573269367\n",
            "200 -----> Perdida: 0.1687566404711528\n",
            "210 -----> Perdida: 0.16899733137621203\n",
            "220 -----> Perdida: 0.17008672127383845\n",
            "230 -----> Perdida: 0.1612462433738257\n",
            "240 -----> Perdida: 0.15711746646311384\n",
            "250 -----> Perdida: 0.152281140056957\n",
            "260 -----> Perdida: 0.15568497593478817\n",
            "270 -----> Perdida: 0.1486766012806834\n",
            "280 -----> Perdida: 0.14491713332795098\n",
            "290 -----> Perdida: 0.14211645165762693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred,z = F_propagation(X_test,W1,W2)\n",
        "pred = np.array([np.argmax(x) for x in pred])\n",
        "print(\"Accuracy: \",sum(pred==Y_test)/len(pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vk2oOkdwU6g",
        "outputId": "a96642f3-8845-4819-fcef-1f1f381ee90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usando 300 épocas, una tasa de aprendizaje de 1e-2 y un batch size de 10, podemos alzanzar un 100% de precisión en los datos de prueba."
      ],
      "metadata": {
        "id": "XLrUJLFhd6Rm"
      }
    }
  ]
}
